{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>2. Unstructured Data Classification</center></h1>\n",
    "\n",
    "<p style='text-align: center;'> \n",
    "Jishnu Jeevan <br>\n",
    "Department of Computer Science <br>\n",
    "M.Tech Computer and Information Science <br>\n",
    "jishnujeevan@cusat.ac.in <br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center> Assignemt Objective</center></h2>\n",
    "<p style='text-align: justify;'>\n",
    "These 30 scientific articles come from three different domains:<br>\n",
    "PLoS Computational Biology (PLOS)<br>\n",
    "The machine learning repository on arXiv (ARXIV)<br>\n",
    "The psychology journal Judgment and Decision Making (JDM)<br>\n",
    "There are 10 articles from each domain. In addition to the labeled data, this corpus also contains a corresponding set of unlabeled articles. These unlabeled articles also come from PLOS, ARXIV, and JDM. There are 300 unlabeled articles from each domain (again, only the sentences from the abstract and introduction).<br>\n",
    "Our objective is to label each sentence in the unlabelled data set with AIMX, OWNX, CONT, BASE, MISC.<br>\n",
    "Here the training set is smaller than the test set.<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The data set contains the following information: </h2>\n",
    "<p style='text-align: justify;'>\n",
    "    \n",
    "1. labeled_articles<br>\n",
    "It has a total of 30 texts (10 text from each domain PLOS, ARXIV, JDM)<br>\n",
    "Each of the sentence is marked with the following label\n",
    "Aim - AIMX - The specific research goal of the paper<br>\n",
    "Own - OWNX - The author’s own work, e.g. methods, results, conclusions<br>\n",
    "Contrast - CONT - Contrast, comparison or critique of past work<br>\n",
    "Basis - BASE - Past work that provides the basis for the work in the article.<br>\n",
    "Misc - MISC - Any other sentences<br>\n",
    "    \n",
    "2. unlabelled_articles<br>\n",
    "They contain three folder from arxiv_unlabelled, plos_unlabelled, jdm_unlabelled. <br>\n",
    "Each folder contains 300 test files. But these test files don't have there sentence labelled.\n",
    "\n",
    "3. word_list <br>\n",
    "Conaints test files named aim, base, contrast, own, stopwords. Each test file contains the words that are commonly used with the labels aim, base, contrast, own. <br>\n",
    "Stopwords contains the words that don't have any meaning like of, a, and, the etc.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For path manipulation\n",
    "import os\n",
    "\n",
    "# For showing a progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For regular expression\n",
    "import re\n",
    "\n",
    "# For training, testing and splitting of the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# For calculating accuracy\n",
    "from sklearn import metrics, preprocessing\n",
    "\n",
    "#Classification Algorithms \n",
    "from sklearn.model_selection import cross_val_score # Cross validation\n",
    "from sklearn.tree import DecisionTreeClassifier # Decision Tree\n",
    "from sklearn.svm import SVC # C - Suport Vector Classification\n",
    "from sklearn.multiclass import OneVsRestClassifier # One vs rest classifier\n",
    "from sklearn.linear_model import SGDClassifier # Stochastic Gradient Descent\n",
    "from sklearn.neighbors import KNeighborsClassifier # K Neighbours\n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forrest\n",
    "from sklearn.naive_bayes import MultinomialNB # Multinominal Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read the file and seperate the labels and the sentence. \n",
    "### Here a sentence will be of the form: AIMX\"tab\"We show that this is even the case if the model class contains only Bernoulli distributions. \n",
    "### The first word AIMX is the label and the rest that follows after the tab is the sentence. I coudn't display tab, so I wrote it as \"tab\"\n",
    "### Due to the fact that many text documents were not seperated by tab, I had to manually insert tab after the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 1500.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Path of the training data\n",
    "Training_Folder = './SentenceCorpus/labeled_articles'\n",
    "\n",
    "# Labels\n",
    "Training_Label = []\n",
    "\n",
    "# Sentence\n",
    "Training_Matrix = []\n",
    "\n",
    "# Take each file from labeled_articles\n",
    "for file in tqdm(os.listdir(Training_Folder)):\n",
    "        \n",
    "    # Open each file in reaf mode\n",
    "    Text_File = open(Training_Folder + \"/\" + file,'r+')\n",
    "    \n",
    "    # Take each line of the file\n",
    "    for line in Text_File:\n",
    "\n",
    "        # If it starts with ### dont process it\n",
    "        if (line.startswith('###')):\n",
    "            continue\n",
    "\n",
    "        # Reomve other characters like , . | etc with a space \n",
    "        line = re.sub(r'[-*+%$()\\.,/?!><\"&#\\[\\]\\(\\)\\\\]', ' ',line)\n",
    "        \n",
    "        # Split the sentace by tab\n",
    "        lines = line.split(\"\\t\")\n",
    "        #print(lines)\n",
    "        \n",
    "        # Put the sentence into list and convert it to lower case\n",
    "        Training_Matrix.append(lines[1].lower())\n",
    "\n",
    "        # Put the labels in list\n",
    "        Training_Label.append(lines[0])\n",
    "    Text_File.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Now we are going to do pre processing on the words before we feed them to a training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Tokenization - Converting sentence to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the minimum description length principle for online sequence estimation prediction in a proper learning setup is studied\n",
      "\n",
      "['the', 'minimum', 'description', 'length', 'principle', 'for', 'online', 'sequence', 'estimation', 'prediction', 'in', 'a', 'proper', 'learning', 'setup', 'is', 'studied']\n"
     ]
    }
   ],
   "source": [
    "# import the nltk (natural language took kit)\n",
    "import nltk\n",
    "\n",
    "# Import word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create a list to store the tokenized form of each sentence\n",
    "Training_Matrix_tokenized = []\n",
    "\n",
    "# Take every sentence in training matrix\n",
    "for sentence in Training_Matrix:\n",
    "    \n",
    "    # Append the tokenized form of the sentence\n",
    "    Training_Matrix_tokenized.append(word_tokenize(sentence))\n",
    "\n",
    "print(Training_Matrix[0])\n",
    "print(Training_Matrix_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Stop word removal - Frequent words such as ”the”, ”is”, etc. that do not have specific semantic. \n",
    "#### We already have a file which contains all the stop words. We are going to use the words in that file to remove the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of', 'a', 'and', 'the', 'in', 'to', 'for', 'that', 'is', 'on', 'are', 'with', 'as', 'by', 'be', 'an', 'which', 'it', 'from', 'or', 'can', 'have', 'these', 'has', 'such']\n"
     ]
    }
   ],
   "source": [
    "# This is the path of the stop word file\n",
    "Stop_Word_Folder = './SentenceCorpus/word_lists/stopwords.txt'\n",
    "\n",
    "# Create a list to store all the stop words\n",
    "stop_words = []\n",
    "\n",
    "# Open the stop word file\n",
    "Stop_Word_File = open(Stop_Word_Folder,'r+')\n",
    "\n",
    "# Take each word of the stop word file\n",
    "for word in Stop_Word_File:\n",
    "    \n",
    "    # When we take the word from the file, a new line will also be read. Remove that new line\n",
    "    word = word.split('\\n')\n",
    "    \n",
    "    # Append only the word, and not the new line to the list of stop words\n",
    "    stop_words.append(word[0])\n",
    "    \n",
    "# Close the file\n",
    "Stop_Word_File.close()\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now remove all the stop words from the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['minimum', 'description', 'length', 'principle', 'online', 'sequence', 'estimation', 'prediction', 'proper', 'learning', 'setup', 'studied']\n"
     ]
    }
   ],
   "source": [
    "# Create a list to store all the words that are not tokens\n",
    "Training_Matrix_Stop_Words_Removed = []\n",
    "\n",
    "# Take each row of the training matrix tokenized\n",
    "for sentence in Training_Matrix_tokenized:\n",
    "    \n",
    "    # Create a list to store all the words that are not stop words\n",
    "    tokens = []\n",
    "    \n",
    "    # Take each word of the sentence in the row of the training matrix tokenized\n",
    "    for word in sentence:\n",
    "        \n",
    "        # If the  word is not not a stop word\n",
    "        if word not in stop_words:\n",
    "            \n",
    "            # Append it to the token list\n",
    "            tokens.append(word)\n",
    "    \n",
    "    # Append the stop word removed row to the new training matrix\n",
    "    Training_Matrix_Stop_Words_Removed.append(tokens)\n",
    "\n",
    "print(Training_Matrix_Stop_Words_Removed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c. Stemming — words are reduced to a root by removing inflection through dropping unnecessary characters, usually a suffix.\n",
    "#### The stemmed form of studies is: studi\n",
    "#### The stemmed form of studying is: study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['minimum', 'descript', 'length', 'principl', 'onlin', 'sequenc', 'estim', 'predict', 'proper', 'learn', 'setup', 'studi']\n"
     ]
    }
   ],
   "source": [
    "# Import PorterStemmer from nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Initialize porter stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Create a list to store all the stemmed words\n",
    "Training_Matrix_Stemmed = []\n",
    "\n",
    "# Take each row of the training matrix, that had the stop words removed\n",
    "for sentence in Training_Matrix_Stop_Words_Removed:\n",
    "    \n",
    "    # Create a list to store the stemmed version of each word\n",
    "    stems = []\n",
    "    \n",
    "    # Take each word of the sentence, in the row of the training matrix stop word removed\n",
    "    for word in sentence:\n",
    "        \n",
    "        # Append the stemmed version of the word to the stem list\n",
    "        stems.append(porter.stem(word))\n",
    "    \n",
    "    # Append the stemmed word list to new training matrix\n",
    "    Training_Matrix_Stemmed.append(stems)\n",
    "\n",
    "print(Training_Matrix_Stemmed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3d - Count vectorizer - Convert a collection of text documents to a matrix of token counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For creating a data frame\n",
    "import pandas as pd\n",
    "\n",
    "# create a dataframe using texts and lables\n",
    "trainDF = pd.DataFrame()\n",
    "\n",
    "# If we want it properly work, the Training_Matrix_Stemmed contain the following:\n",
    "# ['minimum','descript','length','principl','onlin','sequenc','estim','predict','proper','learn','setup','studi']\n",
    "# This is a sentence that has been tokenized, removed of stop words and stemmed\n",
    "# For proper processing as a data frame, we need to convert the above list of words into a sentence like format\n",
    "# i.e. 'minimum descript length principl onlin sequenc estim predict proper learn setup studi'\n",
    "\n",
    "# To store all the new sentences\n",
    "training_data = []\n",
    "\n",
    "# Take each row of the Training_Matrix_Stemmed:\n",
    "for sentence in Training_Matrix_Stemmed:\n",
    "    \n",
    "    # Join using space\n",
    "    new_sentence = \" \"\n",
    "    \n",
    "    # Join each word with a space to form a sentence\n",
    "    new_sentence = new_sentence.join(sentence) \n",
    "    \n",
    "    # Append it to new training matrix\n",
    "    training_data.append(new_sentence)\n",
    "\n",
    "trainDF['text'] = training_data\n",
    "trainDF['label'] = Training_Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "X_train, X_test, y_train, y_test = train_test_split(trainDF['text'], trainDF['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do count vectorization on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "X_train_count =  count_vect.transform(X_train)\n",
    "X_test_count =  count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. We are going to use different binary classification algorithms to do classification on the data, and we will evaluate the accuarcy of each algorithm.\n",
    "### The algorithms we are going to use are\n",
    "1. Decision Tree Classifier\n",
    "2. Stochastic gradient descent  Classifier\n",
    "3. SVC\n",
    "4. K Nearest Neighbour\n",
    "5. One vs Rest Classifier\n",
    "6. Random Forrest Classifier\n",
    "7. Multinominal Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Name :  1. Decision Tree Classifier\n",
      "Accuracy :  0.3333333333333333\n",
      "Cross validation score :  [0.3307888  0.36970475 0.35233161]\n",
      "\n",
      "\n",
      "Name :  2. Stochastic Gradient Descent\n",
      "Accuracy :  0.3474358974358974\n",
      "Cross validation score :  [0.32315522 0.37355584 0.38341969]\n",
      "\n",
      "\n",
      "Name :  3. C - Support Vector Classifier\n",
      "Accuracy :  0.2935897435897436\n",
      "Cross validation score :  [0.30916031 0.31065469 0.3134715 ]\n",
      "\n",
      "\n",
      "Name :  4. K Nearest Neighbour\n",
      "Accuracy :  0.3628205128205128\n",
      "Cross validation score :  [0.31806616 0.36713736 0.32901554]\n",
      "\n",
      "\n",
      "Name :  5. One vs Rest Classifier\n",
      "Accuracy :  0.37435897435897436\n",
      "Cross validation score :  [0.34732824 0.40693196 0.38212435]\n",
      "\n",
      "\n",
      "Name :  6. Random Forest Classifier\n",
      "Accuracy :  0.3243589743589744\n",
      "Cross validation score :  [0.34860051 0.37098845 0.34715026]\n",
      "\n",
      "\n",
      "Name :  7. Multinominal Naive Bayes\n",
      "Accuracy :  0.4166666666666667\n",
      "Cross validation score :  [0.37913486 0.40051348 0.38341969]\n"
     ]
    }
   ],
   "source": [
    "# To surpress warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a dictionary to find out the best classifier using accuracy score\n",
    "ranking = {}\n",
    "\n",
    "# We are going to be using the following classifiers and doing a comparision study\n",
    "classifiers = {\n",
    "                '1. Decision Tree Classifier':DecisionTreeClassifier(),\n",
    "                '2. Stochastic Gradient Descent':SGDClassifier(max_iter=1000, tol=1e-3), \n",
    "                '3. C - Support Vector Classifier':SVC(gamma='auto'), \n",
    "                '4. K Nearest Neighbour':KNeighborsClassifier(8),   \n",
    "                '5. One vs Rest Classifier': OneVsRestClassifier(SVC(kernel='linear')), \n",
    "                '6. Random Forest Classifier': RandomForestClassifier(),\n",
    "                '7. Multinominal Naive Bayes': MultinomialNB()\n",
    "               }\n",
    "\n",
    "# Take each classifier from list\n",
    "for Name, classifier in classifiers.items():\n",
    "    # Fit the model using the training set\n",
    "    classifier.fit(X_train_count ,y_train)\n",
    "    \n",
    "    # Find out the predicion using test set\n",
    "    y_predicted = classifier.predict(X_test_count)\n",
    "    \n",
    "    # Find out the accuracy using the y test set and perdicted valur of y\n",
    "    accuracy = metrics.accuracy_score(y_test,y_predicted)\n",
    "    \n",
    "    # Cross validaion score\n",
    "    score = cross_val_score(classifier, X_train_count, y_train, cv=3)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\")\n",
    "    print(\"Name : \", Name)\n",
    "    print(\"Accuracy : \", accuracy)\n",
    "    print(\"Cross validation score : \", score)\n",
    "       \n",
    "    # Add the name of classifier and accuracy score to dictionary\n",
    "    ranking[Name] = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. The aglorithms that perform well, accorging to there accuracy score are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "7. Multinominal Naive Bayes ; 0.4166666666666667\n",
      "5. One vs Rest Classifier ; 0.37435897435897436\n",
      "4. K Nearest Neighbour ; 0.3628205128205128\n",
      "2. Stochastic Gradient Descent ; 0.3474358974358974\n",
      "1. Decision Tree Classifier ; 0.3333333333333333\n",
      "6. Random Forest Classifier ; 0.3243589743589744\n",
      "3. C - Support Vector Classifier ; 0.2935897435897436\n"
     ]
    }
   ],
   "source": [
    "# Sort the dictionary 'ranking' accoriding to highest accuracy\n",
    "print(\"\\n\")\n",
    "ranking_sorted = sorted(ranking.items(),  reverse = True, key=lambda x: x[1]) # This returns a tuple, not a dictinary\n",
    "for k,v in ranking_sorted:\n",
    "    print(k, \";\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Now from the accuracy score we can see that Multi nominal naive bayes perfomed well. Now we can use this to do predictions on unlabelled data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MISC' 'MISC ' 'AIMX' 'MISC' 'MISC' 'OWNX ' 'MISC ' 'MISC ' 'MISC '\n",
      " 'MISC ' 'OWNX' 'MISC ' 'MISC ' 'OWNX' 'MISC' 'MISC ' 'MISC ' 'MISC '\n",
      " 'MISC ' 'MISC ' 'AIMX ' 'MISC' 'MISC ' 'MISC' 'MISC ' 'MISC' 'MISC'\n",
      " 'MISC ' 'AIMX' 'OWNX' 'MISC ' 'MISC ' 'MISC ']\n"
     ]
    }
   ],
   "source": [
    "# Create a multi nominal naive bayes\n",
    "model =  MultinomialNB()\n",
    "\n",
    "# Train the model again\n",
    "model.fit(X_train_count ,y_train)\n",
    "\n",
    "# Specify the path of the test data\n",
    "path = './SentenceCorpus/unlabeled_articles/arxiv_unlabeled/1.txt'\n",
    "\n",
    "# Create a variable to store the sentence\n",
    "X_test = []\n",
    "\n",
    "# Open each file in reaf mode\n",
    "Text_File = open(path,'r+')\n",
    "    \n",
    "# Take each line of the file\n",
    "for line in Text_File:\n",
    "\n",
    "    # If it starts with ### dont process it\n",
    "    if (line.startswith('###')):\n",
    "        continue\n",
    "\n",
    "    # Reomve other characters like , . | etc with a space \n",
    "    line = re.sub(r'[-*+%$()\\.,/?!><\"&#\\[\\]\\(\\)\\\\]', ' ',line)\n",
    "\n",
    "    # Put the sentence into list and convert it to lower case\n",
    "    X_test.append(line.lower())\n",
    "\n",
    "# Close the file\n",
    "Text_File.close()\n",
    "\n",
    "# Convert it to a pandas data frame\n",
    "import pandas as pd\n",
    "testDF = pd.DataFrame()\n",
    "testDF['text'] = X_test\n",
    "\n",
    "# Convert it to count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "X_test_count =  count_vect.transform(X_test)\n",
    "\n",
    "# Make prediction\n",
    "y_predicted = model.predict(X_test_count)\n",
    "\n",
    "# Print the prediction\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
